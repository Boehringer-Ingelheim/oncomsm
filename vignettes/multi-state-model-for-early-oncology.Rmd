---
title: "multi-state-model-for-early-oncology"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{multi-state-model-for-early-oncology}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r knitr-options, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.height = 7/1.61,
  fig.align = "center"
)
```

```{r setup}
library(bhmbasket.predict)
library(dplyr, warn.conflicts = FALSE)
library(ggplot2)
```



# Background

**tl;dr:** *Tumor response data is often treated as binary endpoint in early
oncology trials (objective response or progression free survival at x months). 
This ignores the fact that these are time-to-event endpoints. 
Instead, the multi-state characteristics of the data can be exploited to 
reduce bias at interim analyses and drive event prediction for probability of 
success calculations.*

In early oncology trials, (objective tumor) response based on the RECIST
criteria is often used as primary endpoint to establish the activity of
a treatment. Response is often treated as binary variable although it is
a delayed event endpoint. At the final analysis, this simplification is
of little concern since all individuals tend to be followed up long
enough to justify the assumption that non-responders are unlikely to
still become responders. This approach is often plausible since
individuals with tumors not responding to treatment ultimately progress.
However, when continuously monitoring such a trial, the assumption of
sufficient follow-up is no longer fulfilled and a simple binary analysis
may be biased.

In this vignette, the problem is addressed by extending the statistic
binary response model to a three-state model for "stable", "response", and
"progression or death".
The respective transition numbers are given in the graph below.

```{r msm-1, eval=TRUE, echo=FALSE}
DiagrammeR::mermaid("
graph LR
  stable -- 1 --> response
  response -- 2 --> progression[progression or death]
  stable -- 3 --> progression[progression or death]
", height = 200)
```

Once someone becomes a responder, this model only allows a transition to
the combined progression or death state.
This means that progression and death are not modeled as competing events and
simplifies analysis greatly over a model like the one below:

```{r msm-2, eval=TRUE, echo=FALSE}
DiagrammeR::mermaid("
graph LR
  stable -- 1 --> response
  response -- 2a --> progression
  response -- 2b --> death
  progression -- 4 --> death
  stable -- 3a --> progression
  stable -- 3b --> death
", height = 200)
```

Coming back to the simpler model, there is still a competing event issue when
starting from 'stable'.
Often, the convenient hazard-based approach is used to model multistate 
situations like this. The hazard-based approach is intuitive, especially when 
considering non-parametric estimation, and co variate inclusion is well-understood.
However, a hazard-based approach has several disadvantages:

1. The hazard scale is difficult to interpret since it is a momentary risk, not a probability. 
This leads to problems with prior specification in a Bayesian setting.
The Bayesian approach is, however, particularly useful in the early development 
process since it allows to augment data with prior opinion or evidence and thus 
improve accuracy.

2. A hazard based, non-Markov multi-state model leads to intractable expressions 
for the implicitly given transition probabilities. Hence they need to be 
calculated by simulation which makes the model less convenient to work with if
transition probabilities a or of primary interest. Since the (objective) response 
rate often plays an important role in the analysis of early oncology trials,
this is a disadvantage. 

An alternative framework to model multi-state data is using mixture models.
For details, see {ref} [link to flexsurv and covid paper].
Here, we describe the concrete application to he simplified "stable", 
"response", "progression" model.
The approach is similar to [gaelle paper].

Assuming that the transition process can be described as a semi-Markov process
is one way of rendering modeling tractable. 
This means that the time to the next transition only depends on the time
already spent in a state, not on prior jumps. 
Additionally, it is assumed that the transition times between states conditional
on both originating and target state can be described by Weibull distributions. 
This parametric family encompasses the exponential distribution with constant
transition rates as special cases but also allows increasing or decreasing 
hazards over time. 

Transition 2 is fully characterized by the corresponding Weibull distribution
since there is no competing event.

$$
\operatorname{Pr}[2|response] = 1 \\
dt_{response \ \to \ \cdot} \sim \operatorname{Weibull}(shape_2, scale_2) \\
\operatorname{Pr}[1|stable] = p \\
dt_{s \ \to \ x} | x = r \sim \operatorname{Weibull}(shape_2, scale_2) \\
dt_{s \ \to \ x} | x = p \sim \operatorname{Weibull}(shape_3, scale_3) \\
$$
This model implies that 
$$
dt_{s \ \to \ x}  \sim p \operatorname{Weibull}(shape_1, scale_1)  + (1 - p) \operatorname{Weibull}(shape_2, scale_2)\\
$$
hence, it can be seen as a mixture model. 

Instead of parameterizing the Weibull distributions directly via shape and scale,
shape and median-time-to-next-event are used.
The scale parameter can then be recovered via the relationship 

$$
\operatorname{scale} = \frac{\operatorname{median-time-to-next-event}}{\log(2)^{1/\operatorname{shape}}} . 
$$

The following priors are used:

1. a normal prior for the log-odds of response $\operatorname{logit}(p) \sim \mathcal{N}(\mu_p,\sigma_p^2)$
2. a truncated normal prior for the median-time-to-next-event of each transition $\operatorname{median-time-to-next-event}_{\,i} \sim \mathcal{N}(\mu_i,\sigma_i^2)[\,0,\infty)$
3. a flat prior on the shape of each transition $\operatorname{shape}_{\,i} \sim \operatorname{Uniform}(a_i. n_i)$
4. it is assumed that the observation process (visit spacing) is fixed, e.g. every 6 weeks. This is irrelevant for inference, but important for sampling forward from the model since it cannot be assumed that transition times are observed exactly but only at the regular visits. 

We treat recruitment times as independent of outcome. 
They can be modeled separately if required.


# Example 

## Specifying the model

We assume a time-scale of months.

```{r specify-model, eval = TRUE}
mdl <- create_srp_model(
    # names of the arms/groups
    group_id = c("control", "intervention"),
    # per-group logodds of response|stable
    logodds_mean =  c(logodds(.25), logodds(.5)),
    logodds_sd = c(.75, .75),
    # m[i,j] is the median time to next event for group i and transition j
    median_time_to_next_event = matrix(c(
      3, 2, 6,
      2, 8, 12
    ), byrow = TRUE,  nrow = 2, ncol = 3),
    # fixed standard deviation of the prior for all median times
    median_time_to_next_event_sd = matrix( 
      1, byrow = TRUE,  nrow = 2, ncol = 3
    ),
    # uniform prior over the shape parameter, difficult to identify, 
    # better keep it tight to avoid issues with the sampler
    shape_min = matrix( 
      .75, byrow = TRUE,  nrow = 2, ncol = 3
    ),
    shape_max = matrix( 
      2, byrow = TRUE,  nrow = 2, ncol = 3
    ),
    # the visit interval
    visit_spacing = c(1.2, 1.2)
  )
# TODO: implement a nice summary / print function here
```


## Prior checks

First we plot the cumulative distribution functions (CDF) of the 
time-to-next-event over the first 36 (months) and the (empirical) CDF of the 
response probabilities per group.
These are based on a sample drawn from the prior distribution of the model.

```{r plotting-the-prior, eval = TRUE}
library(patchwork)
plot(mdl, dt = c(0, 36), seed = 36L)
```

Next, we draw samples from the prior-predictive distribution of the model.
We sample 100 trials with 30 individuals per arm.

```{r prior-predictive, eval = TRUE}
tbl_prior_predictive <- sample_prior_predictive(
  mdl, 
  n_per_arm = c(30L, 30L), 
  nsim = 100, 
  seed = 36457
) 

# estimate of the response rates
tbl_prior_predictive %>% 
  filter(from == "stable") %>% 
  group_by(group_id) %>% 
  summarize(p_response = sum(to == "response")/n())

# crude approximation of the median jump times from "stable"
tbl_prior_predictive %>%  
  filter(from == "stable") %>% 
  group_by(group_id, from, to) %>% 
  summarize(t_jump_approx = median(t_min + t_max)/2, .groups = "drop") 
```

The generated data can also be visualized in swimmer plots

```{r plot-preior-predictive, fig.height=6, eval = TRUE}
# TODO: improve sorting
plot_mstate(mdl, tbl_prior_predictive %>% filter(iter == 1))
```



# Inference

Usually, data will come in the form of individual visits, not yet in the
form of interval censored transitions.
We can mimic this for the sake of example by converting one of our
prior predictive samples to visit data.
Note that the conversion is not exact since the mstate format does not contain
information about potential visits between the interval boundaries.

```{r}
tbl_visits <- tbl_prior_predictive %>% 
  filter(iter == 1) %>% 
  mstate_to_visits(mdl, .)

print(tbl_visits, n = 25)
```

Next, we add recruitment times.

```{r}
# sample from poisson process
set.seed(31532)
tbl_sot <- tbl_visits %>% 
  select(
    subject_id, 
    group_id
  ) %>% 
  distinct() %>% 
  arrange(runif(n())) %>% # permute groups
  mutate(
    t_sot = cumsum(rexp(n = n(), rate = 3)) # poisson recruitment process
  )

# add to visit times
tbl_visits <- tbl_visits %>% 
  left_join(tbl_sot, by = c("subject_id", "group_id")) %>% 
  mutate(t = t + t_sot) %>% 
  select(-t_sot)

# convert back to mstate format and plot
tbl_visits %>% 
  visits_to_mstate(start_state = "stable", absorbing_states = "progression") %>% 
  plot_mstate(mdl, ., relative_to_sot = FALSE)
```

If we now cut the data at 15 months after the first SoT, we can imitate a
hypothetical interim analysis.
Since there are still some individuals waiting to be recruited, we need to
add them back in such that their future trajectories can be imputed.
We do this by adding their hypothetical first visits at 15 months. 
If we want to explore different recruitment scenarios, we can then modify the
SoT of each individual with SoT = 15 months.

```{r}
tbl_visits_interim <- tbl_visits %>% 
  filter(t <= 15) 

tbl_visits_interim <- bind_rows(
    tbl_visits_interim,
    tibble(
      subject_id = setdiff(unique(tbl_visits$subject_id), unique(tbl_visits_interim$subject_id)),
      t = 15,
      state = "stable"
    ) %>% 
    left_join(
      distinct(select(tbl_visits, subject_id, group_id)), 
      by = "subject_id"
    )
  )

# convert to mstate plot
tbl_mstate_interim <- visits_to_mstate(
  tbl_visits_interim, 
  start_state = "stable", 
  absorbing_states = "progression", 
  now = 17
)
plot_mstate(mdl, tbl_mstate_interim, relative_to_sot = FALSE)
```

```{r}
tbl_mstate_interim %>% filter(row_number() <= 2)
sample_posterior(mdl, tbl_mstate_interim %>% filter(row_number() == 1), refresh = 10)
```



# Session info

```{r session-info}
sessionInfo()
```

