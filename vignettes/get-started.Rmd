---
title: "Get Started"
author: Kevin Kunzmann
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Get Started}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r vignette-setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 6,
  fig.align = "center"
)
```

```{r setup}
library(bhmbasket.predict)
library(dplyr, warn.conflicts = FALSE)
library(ggplot2)
```


# Background

In early oncology trials, (objective tumor) response based on the RECIST criteria
is often used as primary endpoint to establish the activity of a treatment.
Response is commonly analyzed as binary variable. 
This implicitly assumes that there is no relevant censoring at the end of a trial
since a response either occurs relatively early after onset of 
treatment or not at all.
This is plausible if individuals with tumors not responding to treatment 
ultimately progress.
However, when continuously monitoring such a trial, 
the assumption of sufficient follow-up is no longer fulfilled and a simple 
binary analysis may be biased.



## A mixture-cure rate model for binary response data

A mixture-cure-rate model decouples the modeling of an event probability $p$
from the corresponding time-to-event distribution. 
This is an alternative to multistate modeling via cause specific hazards (??? Jackson paper).
The advantage for response-rate modeling lies in the fact that the response 
probability is modeled directly and that the same prior assumptions which are 
valid for a Binomial model can be used as part of the prior for a model also 
encompassing the time-to-response.

A mixture-cure-rate-type model for binary response assumes that the population 
is a mixture of (ultimate) responders and (ultimate) non-responders.
In this context the "survival function" defines the proportion of a population 
that has not experienced a response up to time $dt$ (time since first visit of individual).
The "survival function" of the non-responder subpopulation is constantly $1$
since, by definition, this subpopulation never experiences the event 
in question (a response).
The survival function of the responders, $S_r(dt)$, i.e. the fraction of responders who did not experience a response up to $dt$, needs to be modeled.
Here we chose a Weibull distribution. 
A complex (semi- or non-parametric) survival function would be next to impossible to identify in the small data settings usually encountered in early phase trials and the Weibull distribution is flexible enough to allow for increasing or decreasing hazards over time.
The population survival function is then a mixture of the survival functions of the respective subpopulation weighted by the response probability (interpreted as proportion of responders in the overall population):
$$S(dt) = (1 - p)\,\underbrace{1}_{\hspace{-1.5cm}\text{non-responder}} + p\,\underbrace{S_r(dt)}_{\text{responder}}$$
It is clear that $p$ has the same interpretation as the response probability
in a binomial model and any prior assumption on $p$ can be used interchangeably
between the two models. 
The mixture-cure-rate approach is thus a direct extension of the binomial model.
However, since time-to-response is also modeled, it is possible include individuals who are still "at risk" of experiencing a response due to short 
follow-up in a principled way.
Additionally, the explicit modeling of time-to-response allows to predict or 
impute not only whether an individual at risk will ultimately become a responder 
or not, but also when the response might occur. 



## Recruitment 

For inference about the absolute time of response since the start of a trial
for individuals who have not yet been recruited, 
a model for recruitment time $t$ relative to the start of the trial is also
required.
Here we choose a simple homogeneous Poisson process to model recruitment, i.e.,
the recruitment rate is constant in time.
We assume that $t$ and thus $dt$ are on the scale of months.


## Observation process

The exact time of a response is not observed since the status is only assessed
during (scheduled) visits. 
This intermittently observed panel data means that the response times are
interval censored between the last non-response visit and the first 
response visit.
Note that concepts like confirmed response can be reflected in the definition of
"response" if needed.

We furthermore assume a fixed visit spacing (on the scale of months) i.e. that
future visits occur with perfect regularity and are not stochastic. 



## Decision criterion

Assuming that the formal decision at the end of a trial is based on a function $f$ of the sufficient statistic of the number of responders $R$ and the sample size $n$,
w.l.o.g. a success can be defined as 
$$success := f(R, n) \geq c$$
where $c$ is a critical value that has to be chosen prospectively.
For instance, the observed response rate $f(R,n) = R/n)$ could be used.

Alternatively, more complex decision rules could be based on the Bayesian 
posterior distribution of the response rate. 
This is particularly attractive in basket trial with multiple arms where 
Bayesian hierarchical models allow borrowing between arms (shrinkage).
The decision of a single arm then depends on all $R_i,n_i$ with $i=1\ldots m$
being the arm-indices.
Such approaches are implemented in bhmbasket and examples for both 
approaches are discussed further below.
Note however, that the `bhmbasket.predict` package currently assumes 
independence of parameters between arms for sake of simplicity and thus does not
make use of shrinkage between arms.



## Probability of Success

Once "success" is clearly defined, the unconditional probability of observing a
"success" is the Probability of Success (PoS)
$$PoS := Pr["success"] := Pr[f(R,n)\geq c]$$
This probability depends on the unknown parameters and thus requires the
specification of prior distributions over 

1. the response rate $p$
2. the Weibull shape and scale
3. the recruitment rate

as well as a minimal follow-up time for each individual to determine when 
the trial ends and a planned spacing between individual visits.

The Probability of Success is different from power which conditions on a
particular effect size.
Also note that Probability of Success can also be defined as 
$Pr[f(R,n)\geq c, p \geq p_{relevant}]$, i.e., the joint probability of
observing a success and the response rate actually being relevant.
Since the latter event is not observable and the difference to PoS as defined above is limited by the false positive rate the latter definition is rarely used [paper].

In a Bayesian model PoS can be updated as data accrues over time.
This allows to interpolate between the a priori planning PoS and 0 (no success at end) or 1 (success) at the end of the trial. 
To this end, the prior is updated with the data observed in the trial and
PoS is computed using this updated prior (posterior).
It is this "dynamic PoS" that should take censoring and delayed responses into 
account via, e.g., the mixture-cure-rate-type model outlined above.



# Example data

We consider the example of a three-arm basket trial. 
`bhmbasket.predict` provides a function to simulate visit data under a simple
model (independence of arms).

```{r generate-example-data}
tbl_visits <- generate_visit_data(
    group_id = c("A", "B", "C"),
    n = c(40, 30, 30), # sample size per arm
    response_rate = c(0.33, .4, .2),
    recruitment_rate = c(2, 1.5, 1.5), # per month
    visit_spacing = c(1.2, 1.2, 1.2), # how many months between visits?
    max_duration = 48, # in months
    responder_weibull_scale = rep(4, 3),
    responder_weibull_shape = rep(5, 3),
    nonresponder_weibull_scale = rep(4, 3),
    nonresponder_weibull_shape = rep(2, 3)
  )

tbl_visits
```

Here, `eof` stands for end of follow up and indicates whether a particular visit
is a patient's last visit.
This information needs to be stored explicitly to identify whether an individual
is still "at risk" of a response at a certain point in time.
Three states `S`(table), `P`(rogressive), `R`(responding) characterize the 
status of an individual at each visit.
An overview plotting function for longitudinal visit-type data is provided.
The simulated data does not contain post-response follow-up.
Duration-of-response analyses are thus not currently implemented.

```{r plot-visit-data}
# epsilon is thickness of last visit state as fraction of a months
plot_visits(tbl_visits, epsilon = 15/30)
``` 


# Specifying a model

The model specification requires input about the prior hyperparameters for 
each arm.
The response rate is parameterized in terms of its log-odds.
All parameters accept normal priors (mean and standard deviation).
The log-odds additionally allow the specification of a minimum and maximum (i.e. truncated normal prior on log-odds).
The prior for the recruitment rate is specified on the log scale and a maximal
waiting time between recruitment of individuals can be given 
(i.e. truncating the exponential waiting time).

The sub-models for recruitment and time-to-response can be specified separately.

```{r prior-specification}
mdl_tte <- independent_mixture_cure_rate_model(
  group_id = c("A", "B", "C"),
  logodds_mean = logodds(c(.4, .2, .2)),
  logodds_sd = c(1, 1, 1),
  logodds_max = logodds(c(.6, .6, .6)),
  shape_mean = c(4, 4, 4),
  shape_sd = c(2, 2, 2),
  median_time_to_response_mean = c(5, 5, 5),
  median_time_to_response_sd = c(1, 1, 1),
  max_time_to_response = c(12, 12, 12),
  visit_spacing = c(1.2, 1.2, 1.2)
)

mdl_recruitment <- independent_poisson_recruitment_model(
  group_id = c("A", "B", "C"),
  log_monthly_rate_mean = log(c(2, 1, 1)),
  log_monthly_rate_sd = c(.25, .25, .25),
  maximal_recruitment_interval = c(3, 3, 3)
)

mdl <- model(mdl_tte, mdl_recruitment)
```



## Prior checks

The prior assumptions can be visualized directly using a custom
`plot` method when specifying per-arm sample sizes. 

```{r prior-sample}
plot(mdl, n = c(40L, 30L, 30L))
```



# Computing Probability of Success

Probability of Success can be computed if a decision criterion
is defined. 
A simple criterion could be based on a threshold on the number of responses or an observed response rate of e.g. 40%. 

```{r compute-planning-pos}
# sample from the prior predictive distribution
tbl_prior_predictive_samples <- draw_samples(
  mdl, 
  n = c(40L, 30L, 30L), 
  seed = 42L
)

# calculate number of responses per iteration 
tbl_prior_predictive_samples %>% 
  group_by(iter, group_id) %>% 
  summarize(
    n = n(),
    r = sum(t_recruitment + dt2 < Inf), # a response is observed if the response time point if finite
    .groups = "drop"
  ) %>% 
  mutate(
    `response rate` = r/n,
    success = case_when(
      group_id == "A" ~ `response rate` >= 0.3,
      group_id == "B" ~ `response rate` >= 0.3,
      group_id == "C" ~ `response rate` >= 0.3
    )
  ) %>% 
  group_by(group_id) %>% 
  summarize(
    PoS = mean(success) # calculate observed frequency of "successes
  ) %>% 
  rename(arm = group_id) %>% 
  knitr::kable(
    caption = "Probability of success by arm."
  )
```



## Expected power

Adjusting the truncation parameters for the log-odds of the 
response probability can be used to sample from the model
conditional on the response probabilities restricted to the respective interval.
For instance, one could sample conditional on a response rate
at least as large as a target value of 0.3.
This allows to investigate the operating properties conditional on 
the alternative - although here the alternative is not a point in the
parameter space.

TODO: mean can be outside of min/max interval failing initialization

```{r condition-on-effect-size}
# change hyperparameters
mdl$tte_model$logodds_min <- logodds(c(.2, .2, .2))

# sample and plot again
plot(mdl, n = c(40L, 30L, 30L))
```

The corresponding success probability is the expected power.

```{r compute-planning-expected-power}
tbl_prior_predictive_samples <- draw_samples(
  mdl, 
  n = c(40L, 30L, 30L), 
  seed = 42L
)

tbl_prior_predictive_summary <- tbl_prior_predictive_samples %>% 
  group_by(iter, group_id) %>% 
  summarize(
    n = n(),
    r = sum(t_recruitment + dt2 < Inf),
    .groups = "drop"
  ) 

tbl_prior_predictive_summary %>% 
  mutate(
    `response rate` = r/n,
    success = case_when(
      group_id == "A" ~ `response rate` >= 0.3,
      group_id == "B" ~ `response rate` >= 0.3,
      group_id == "C" ~ `response rate` >= 0.3
    )
  ) %>% 
  group_by(group_id) %>% 
  summarize(
    `Expected power` = mean(success)
  ) %>% 
  rename(arm = group_id) %>% 
  knitr::kable(
    caption = "Expected power by arm."
  )
```



## Advanced decision boundaries: connection with bhmbasket

...

```{r bhmbasket-integration}
# this value should be larger in practice
nsim_bhmbasket <- 100L

# remove conditioning of response rate (see expected power) 
# by adjusting lower truncation
mdl$tte_model$logodds_min <- logodds(c(.001, .001, .001))

# specify priors for bhmbasket analysis (should be consistent)
params <- bhmbasket::setPriorParametersExNex(
  mu_mean   = logit(0.2), # exchangeable component mean normal prior mean
  mu_sd     = 1, # exchangeable component mean normal prior standard deviation
  tau_scale = 1, # standard deviation of half-normal prior on tau
  mu_j      = logit(c(.4, .2, .2)), # individual components means
  tau_j     = rep(1, 3), # individual components standard deviations
  w_j       = 0.5 # probability of being exchangeable
)

tbl_bhmbasket_posterior_medians <- tbl_prior_predictive_summary %>%
  # convert to bhmbasket trial objects
  group_by(iter) %>%
  summarize(
    trial = list(bhmbasket::createTrial(n, r))
  ) %>% 
  mutate(
    # map a function over all "trial"realizations which computes the
    # posterior medians
    tmp = purrr::map(trial, function(trial) {
        posterior_medians <- suppressMessages(bhmbasket::performAnalyses(
            scenario_list         = trial,
            method_names          = "exnex",
            prior_parameters_list = params,
            n_mcmc_iterations     = nsim_bhmbasket,
            verbose               = FALSE
          )) %>%
          bhmbasket::getEstimates() %>%
          {as.numeric(.$exnex[,"50%"])} # extract posterior median
        tibble(
          group_id = c("A", "B", "C"),
          posterior_median = posterior_medians
        )
      }
    )
  ) %>% 
  select(-trial) %>% 
  tidyr::unnest(tmp)

# combine the posterior medians with the target rates 
# and check the success criterion
tbl_bhmbasket_posterior_medians %>% 
  left_join(
    tribble(
      ~group_id, ~p_target,
            "A",       .3,
            "B",       .3,
            "C",       .3
    ),
    by = "group_id"
  ) %>% 
  group_by(group_id) %>% 
  summarize(
    PoS = mean(posterior_median >= p_target)
  )
```


## Computing dynamic Probability of Success

...

```{r restrict-data-to-interim-at-10-months}
tbl_visits_interim <- filter(tbl_visits, t <= 10) 

plot_visits(tbl_visits_interim)
```

```{r}
# convert data to time-to-response
tbl_tte_interim <- visits_to_tte(tbl_visits_interim)

tbl_tte_interim
```

```{r sample-from-posterior-predictive}
min_follow_up <- 3 # months, follow up of last individual

# mdl$tte_model$shape_sd <- rep(.1, 3)
# 
# mdl$tte_model$logodds_sd <- rep(1, 3)
# mdl$tte_model$logodds_max <- rep(Inf, 3)
# mdl$tte_model$logodds_min <- rep(logodds(.2), 3)
# 
# tmp <- draw_samples(
#     mdl$tte_model,n = c(10, 10, 10), seed = 42L, return_raw_stan_output = TRUE, nsim = 10
#   )
# 
# pairs(tmp, pars = c("p[1]", "shape[1]", "median_time_to_response[1]"))
# 
# tmp <- draw_samples(
#     mdl$tte_model, data = tbl_tte_interim, seed = 42L, return_raw_stan_output = TRUE, nsim = 1000
#   )




tbl_posterior_predictive_summary <- draw_samples(
    mdl, data = tbl_tte_interim, seed = 42L
  ) %>% 
  group_by(iter, group_id) %>%
  tidyr::nest() %>%
  arrange(iter) %>% 
  mutate(
    r = purrr::map_int(data, function(data) {
          with(data,
            sum(t_recruitment + dt2 <= max(t_recruitment) + min_follow_up)
          )
        }
      ),
    n = purrr::map_int(data, nrow)
  ) %>% 
  select(-data)
```

```{r}
tbl_bhmbasket_interim_posterior_medians <- tbl_posterior_predictive_summary %>%
  # convert to bhmbasket trial objects
  group_by(iter) %>%
  summarize(
    trial = list(bhmbasket::createTrial(n, r))
  ) %>% 
  mutate(
    # map a function over all "trial"realizations which computes the
    # posterior medians
    tmp = purrr::map(trial, function(trial) {
        posterior_medians <- suppressMessages(bhmbasket::performAnalyses(
            scenario_list         = trial,
            method_names          = "exnex",
            prior_parameters_list = params,
            n_mcmc_iterations     = nsim_bhmbasket,
            verbose               = FALSE
          )) %>%
          bhmbasket::getEstimates() %>%
          {as.numeric(.$exnex[,"50%"])} # extract posterior median
        tibble(
          group_id = c("A", "B", "C"),
          posterior_median = posterior_medians
        )
      }
    )
  ) %>% 
  select(-trial) %>% 
  tidyr::unnest(tmp)
```

```{r}
tbl_bhmbasket_posterior_medians %>% 
  left_join(
    tribble(
      ~group_id, ~p_target,
            "A",       .3,
            "B",       .3,
            "C",       .3
    ),
    by = "group_id"
  ) %>% 
  group_by(group_id) %>% 
  summarize(
    PoS = mean(posterior_median >= p_target)
  )
```



# Session info

```{r session-info}
sessionInfo()
```
